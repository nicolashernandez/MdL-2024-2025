{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM9Wtuut0/tYwSx+LaCye07",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolashernandez/MdL-2024-2025/blob/main/model_parameters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercice : Nombre de paramètres d'un modèle\n",
        "\n",
        "En lisant les papiers sources et éventuellement en jetant un oeil aux implémentations, êtes-vous à même de vérifier que le nombre de paramètres de BERT_BASE [1] est de 110 millions de paramètres et que GPT-2 [2] est bien de 1,5 milliards de paramètres ?\n",
        "\n",
        "* [1] https://github.com/google-research/bert et Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n",
        "* [2] https://github.com/openai/gpt-2 et https://openai.com/index/gpt-2-1-5b-release/"
      ],
      "metadata": {
        "id": "H7n5H_M93Kyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le code suivant initie un modèle BERT implémenté en PyTorch et interfacé par la bibliothèque `transformers` de huggingface."
      ],
      "metadata": {
        "id": "5bNBJzNfeNUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "jg1b3Z6r3XyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un affichage du modèle via l'instruction `print` permet d'expliciter ses composants, leurs combinaisons ainsi que la `shape` (taille) des tenseurs.\n"
      ],
      "metadata": {
        "id": "Yb9bwh4Cehx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (model)"
      ],
      "metadata": {
        "id": "EiAZHpzg4eIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 : taille du modèle BERT\n",
        "\n",
        "**TODO** Calculer la taille du modèle en appliquant les principes suivants\n",
        "* le nombre de paramètre d'une couche linéaire est égal au produit de $features_{in} * features_{out} + param_{bias}$ où $param_{bias}$ correspond à features_{out} si `bias=True`\n",
        "* sommer le nombre de paramètres des couches adjacentes\n",
        "* multiplier le nombre de paramètres par le facteur de combinaison quand les couches sont déclarées en liste\n",
        "\n",
        "Garder la trace du total de nombre de paramètres par composant. Vous pouvez utiliser un [tableur](https://lite.framacalc.org) pour ce faire.\n"
      ],
      "metadata": {
        "id": "VUF4t4g8fSYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'instruction `summary` du package [torch-summary](https://pypi.org/project/torch-summary/) résume un modèle PyTorch donné en parammètre. Les informations du résumé inclut:\n",
        "1. Layer names,\n",
        "2. input/output shapes,\n",
        "3. kernel shape,\n",
        "4. number of parameters,\n",
        "5. number of operations (Mult-Adds)\n"
      ],
      "metadata": {
        "id": "19J5tp6zcLBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-summary"
      ],
      "metadata": {
        "id": "zEfxNFJq3a7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary(model)"
      ],
      "metadata": {
        "id": "ZnLlnDVd3Lw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO** Obtenez-vous la même chose ? Sinon dans quelle couche observez-vous des différences ?"
      ],
      "metadata": {
        "id": "zYBW42TfhLP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 : taille de GPT2 ?\n",
        "\n",
        "**TODO** A vous de jouer.\n"
      ],
      "metadata": {
        "id": "j-7_dNjBXTEb"
      }
    }
  ]
}