{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNShD0SmazEJOSDfDWevMlw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolashernandez/MdL-2024-2025/blob/main/model_parameters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercice : Nombre de paramètres d'un modèle\n",
        "\n",
        "En lisant les papiers sources et éventuellement en jetant un oeil aux implémentations, êtes-vous à même de vérifier que le nombre de paramètres de BERT_BASE [1] est de 110 millions de paramètres et que GPT-2 [2] est bien de 1,5 milliards de paramètres ?\n",
        "\n",
        "* [1] https://github.com/google-research/bert et Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n",
        "* [2] https://github.com/openai/gpt-2 et https://openai.com/index/gpt-2-1-5b-release/"
      ],
      "metadata": {
        "id": "H7n5H_M93Kyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le code suivant initie un modèle BERT implémenté en PyTorch et interfacé par la bibliothèque `transformers` de huggingface."
      ],
      "metadata": {
        "id": "5bNBJzNfeNUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "jg1b3Z6r3XyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un affichage du modèle via l'instruction `print` permet d'expliciter ses composants, leurs combinaisons ainsi que la `shape` (taille) des tenseurs.\n"
      ],
      "metadata": {
        "id": "Yb9bwh4Cehx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (model)"
      ],
      "metadata": {
        "id": "EiAZHpzg4eIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 : taille du modèle BERT\n",
        "\n",
        "**TODO** Calculer la taille du modèle en appliquant les principes suivants\n",
        "* le nombre de paramètre d'une couche linéaire est égal au produit de $features_{in} * features_{out} + param_{bias}$ où $param_{bias}$ correspond à features_{out} si `bias=True`\n",
        "* sommer le nombre de paramètres des couches adjacentes\n",
        "* multiplier le nombre de paramètres par le facteur de combinaison quand les couches sont déclarées en liste\n",
        "\n",
        "Garder la trace du total de nombre de paramètres par composant. Vous pouvez utiliser un [tableur](https://lite.framacalc.org) pour ce faire.\n"
      ],
      "metadata": {
        "id": "VUF4t4g8fSYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'instruction `summary` du package [torch-summary](https://pypi.org/project/torch-summary/) résume un modèle PyTorch donné en parammètre. Les informations du résumé inclut:\n",
        "1. Layer names,\n",
        "2. input/output shapes,\n",
        "3. kernel shape,\n",
        "4. number of parameters,\n",
        "5. number of operations (Mult-Adds)\n"
      ],
      "metadata": {
        "id": "19J5tp6zcLBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-summary"
      ],
      "metadata": {
        "id": "zEfxNFJq3a7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary(model)"
      ],
      "metadata": {
        "id": "ZnLlnDVd3Lw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO** Obtenez-vous la même chose ? Sinon dans quelle couche observez-vous des différences ?"
      ],
      "metadata": {
        "id": "zYBW42TfhLP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercice : Generative sampling techniques\n"
      ],
      "metadata": {
        "id": "ugK6EM2wQ-ua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 : Temperature sampling\n",
        "\n",
        "La technique dite de l'échantillonage par température ($t$) permet de lisser (température haute) la distribution des probabilités sur toutes les options ou de concentrer (température basse) la distribution  sur les options les plus probables. Le paramètre $t$ s'applique avant que la distribution soit calculée."
      ],
      "metadata": {
        "id": "0cAGNqpxZLXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ci-dessous la fonction softmax qui permet de calculer la distribution donné x.\n",
        "\n",
        "$\\sigma(x_i) = \\frac{e^{x_{i}/t}}{\\sum_{j=1}^K e^{x_{j}/t}} \\ \\ \\ pour\\ i=1,2,\\dots,K$ et avec temperature $t=1$, pour le softmax par défaut\n",
        "\n",
        "**TODO** implémenter la fonction softmax à l'aide des fonction numpy exp et sum.\n",
        "\n",
        "Soit $x$ un vecteur avec des scores associés à 4 mots désignés par A, B, C et D tel que $x=[1.5, -1.8, 0.9, -3.2]$. Vous devriez retrouvez les résultats suivants :\n",
        "\n",
        "* t=1, softmax = `[0.62704177 0.02312729 0.34412782 0.00570312]`\n",
        "* t=0.5, softmax = `[7.67673394e-01 1.04431835e-03 2.31218783e-01 6.35050642e-05]`\n",
        "\n",
        "Si vous ne trouvez pas, jetez un oeil à la documention de [softmax sous scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.softmax.html)."
      ],
      "metadata": {
        "id": "r8TvYmSoSY02"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2pjsoQcQ95C"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x, t=1):\n",
        "  \"\"\"TODO\"\"\"\n",
        "    return None\n",
        "\n",
        "\n",
        "x = np.array([1.5, -1.8, 0.9, -3.2])\n",
        "print(softmax(x))\n",
        "print(softmax(x, t=0.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO** Observez la distribution suivant les températures. Quelle valeur de $t$ rend le système quasi-déterministe ?"
      ],
      "metadata": {
        "id": "sAjkr9hgeklT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "temperatures =  [5, 2, 1, 0.5, 0.1, 0.01]\n",
        "\n",
        "distribution = list()\n",
        "for t in temperatures:\n",
        "  distribution.append(softmax(x, t=t))\n",
        "\n",
        "df = pd.DataFrame(distribution, columns=['A', 'B', 'C', 'D'], index=temperatures)\n",
        "print (df)\n",
        "\n",
        "ax = df.plot.bar(stacked=True)\n",
        "ax.set_xlabel(\"Temperature\")\n",
        "ax.set_ylabel(\"Probability\")"
      ],
      "metadata": {
        "id": "UV4NCmKAekye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 : \"Top K Sampling\"\n",
        "\n",
        "Pour cette question et la suivante vous utiliserez la démo suivante https://artefact2.github.io/llm-sampling/index.xhtml à la fois pour\n",
        "* observer les valeurs de la distribution initiale\n",
        "* vérifier que vos calculs sont corrects\n",
        "\n",
        "Cela implique de ne pas jouer tout de suite avec la démo...\n",
        "\n",
        "Sélectionner le contexte suivant \"*You will pay for what you have done,\" she hissed, her blade flashing in the moonlight. The battle that ensued _____*\""
      ],
      "metadata": {
        "id": "l4jeJVftReCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "**TODO** Considérer la technique \"Top K\", calculer \"à la main\" les nouveaux scores de probabilités si K=3. Vérifier après coup."
      ],
      "metadata": {
        "id": "r_TLF7UIR2h5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3 : \"Top P Sampling\"\n",
        "\n",
        "Conserver le même contexte que précédemment sur l'espace démo.\n",
        "\n",
        "La technique d'échantillonage avec le Top P implique de garder les tokens les plus probables jusqu'à ce que la somme de leurs probabilités soit plus grande que P.\n",
        "\n",
        "**TODO** Considérer la technique \"Top P\", combien de mots seront considérés si P=0.8. Calculer les nouveaux scores de probabilité de ces mots."
      ],
      "metadata": {
        "id": "quZT_3FShDBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# La loi \"empirique\" de Chinchilla\n",
        "\n",
        "La loi de Chinchilla observée dabs [Training Compute-Optimal Large Language Models (Hoffmann et al. 2022)](https://arxiv.org/abs/2203.15556) recommande d'utiliser 20 fois plus de tokens d'entraînement que le nombre de paramètres du modèle construit.\n"
      ],
      "metadata": {
        "id": "oktnDcB9iVaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question : les modèles actuels\n",
        "\n",
        "Quelle est la tendance des modèles actuels ?\n",
        "\n",
        "La \"Table 3\" de la communication [A Comprehensive Overview of Large Language Models (Naveed et al, 2024)](https://arxiv.org/abs/2307.06435) indique le nombre de paramètres (*No. of Params*) et le nombre de tokens d'entraînement utilisés (*Data/Tokens*) pour de nombreux modèles récents.\n",
        "\n",
        "**TODO** Calculer le nombre de tokens d'entraînement théorique requis selon la loi pour les modèles les plus récents à savoir LLaMA-3.1, Snowflake Arctic, Nemotron-4 340B et DeepSeek-v2. En comparant avec le nombre de tokens réels utilisés, observez-vous une tendance chez les modèles récents ?\n",
        "\n"
      ],
      "metadata": {
        "id": "N8QqRX8kicwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Name             | Nu. of parameters | Chinchilla's pretraining tokens | Real pretraining tokens |\n",
        "|------------------|-------------------|---------------------------------|-------------------------|\n",
        "| LLaMA-3.1        |                   |                                 |                         |\n",
        "| Snowflake Arctic |                   |                                 |                         |\n",
        "| Nemotron-4 340B  |                   |                                 |                         |\n",
        "| DeepSeek-v2      |                   |                                 |                         |\n"
      ],
      "metadata": {
        "id": "PyMz0wwRy5rD"
      }
    }
  ]
}