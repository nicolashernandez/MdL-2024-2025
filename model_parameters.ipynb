{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNErVqZ32IkvebqdLtuPEkk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolashernandez/MdL-2024-2025/blob/main/model_parameters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercice : Nombre de paramètres d'un modèle\n",
        "\n",
        "En lisant les papiers sources et éventuellement en jetant un oeil aux implémentations, êtes-vous à même de vérifier que le nombre de paramètres de BERT_BASE [1] est de 110 millions de paramètres et que GPT-2 [2] est bien de 1,5 milliards de paramètres ?\n",
        "\n",
        "* [1] https://github.com/google-research/bert et Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n",
        "* [2] https://github.com/openai/gpt-2 et https://openai.com/index/gpt-2-1-5b-release/"
      ],
      "metadata": {
        "id": "H7n5H_M93Kyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le code suivant initie un modèle BERT implémenté en PyTorch et interfacé par la bibliothèque `transformers` de huggingface."
      ],
      "metadata": {
        "id": "5bNBJzNfeNUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "jg1b3Z6r3XyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un affichage du modèle via l'instruction `print` permet d'expliciter ses composants, leurs combinaisons ainsi que la `shape` (taille) des tenseurs.\n"
      ],
      "metadata": {
        "id": "Yb9bwh4Cehx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (model)"
      ],
      "metadata": {
        "id": "EiAZHpzg4eIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 : taille du modèle BERT\n",
        "\n",
        "**TODO** Calculer la taille du modèle en appliquant les principes suivants\n",
        "* le nombre de paramètre d'une couche linéaire est égal au produit de $features_{in} * features_{out} + param_{bias}$ où $param_{bias}$ correspond à features_{out} si `bias=True`\n",
        "* sommer le nombre de paramètres des couches adjacentes\n",
        "* multiplier le nombre de paramètres par le facteur de combinaison quand les couches sont déclarées en liste\n",
        "\n",
        "Garder la trace du total de nombre de paramètres par composant. Vous pouvez utiliser un [tableur](https://lite.framacalc.org) pour ce faire.\n"
      ],
      "metadata": {
        "id": "VUF4t4g8fSYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'instruction `summary` du package [torch-summary](https://pypi.org/project/torch-summary/) résume un modèle PyTorch donné en parammètre. Les informations du résumé inclut:\n",
        "1. Layer names,\n",
        "2. input/output shapes,\n",
        "3. kernel shape,\n",
        "4. number of parameters,\n",
        "5. number of operations (Mult-Adds)\n"
      ],
      "metadata": {
        "id": "19J5tp6zcLBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-summary"
      ],
      "metadata": {
        "id": "zEfxNFJq3a7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary(model)"
      ],
      "metadata": {
        "id": "ZnLlnDVd3Lw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO** Obtenez-vous la même chose ? Sinon dans quelle couche observez-vous des différences ?"
      ],
      "metadata": {
        "id": "zYBW42TfhLP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 : taille de GPT2 ?\n",
        "\n",
        "**TODO** A vous de jouer.\n"
      ],
      "metadata": {
        "id": "j-7_dNjBXTEb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# La loi \"empirique\" de Chinchilla\n",
        "\n",
        "La loi de Chinchilla observée dabs [Training Compute-Optimal Large Language Models (Hoffmann et al. 2022)](https://arxiv.org/abs/2203.15556) recommande d'utiliser 20 fois plus de tokens d'entraînement que le nombre de paramètres du modèle construit.\n"
      ],
      "metadata": {
        "id": "oktnDcB9iVaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question : les modèles actuels\n",
        "\n",
        "Quelle est la tendance des modèles actuels ?\n",
        "\n",
        "La \"Table 3\" de la communication [A Comprehensive Overview of Large Language Models (Naveed et al, 2024)](https://arxiv.org/abs/2307.06435) indique le nombre de paramètres (*No. of Params*) et le nombre de tokens d'entraînement utilisés (*Data/Tokens*) pour de nombreux modèles récents.\n",
        "\n",
        "**TODO** Calculer le nombre de tokens d'entraînement théorique requis selon la loi pour les modèles les plus récents à savoir LLaMA-3.1, Snowflake Arctic, Nemotron-4 340B et DeepSeek-v2. En comparant avec le nombre de tokens réels utilisés, observez-vous une tendance chez les modèles récents ?\n",
        "\n"
      ],
      "metadata": {
        "id": "N8QqRX8kicwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Name             | Nu. of parameters | Chinchilla's pretraining tokens | Real pretraining tokens |\n",
        "|------------------|-------------------|---------------------------------|-------------------------|\n",
        "| LLaMA-3.1        |                   |                                 |                         |\n",
        "| Snowflake Arctic |                   |                                 |                         |\n",
        "| Nemotron-4 340B  |                   |                                 |                         |\n",
        "| DeepSeek-v2      |                   |                                 |                         |\n"
      ],
      "metadata": {
        "id": "PyMz0wwRy5rD"
      }
    }
  ]
}